---
layout: single
title:  "실습예제 입니다."
---

*시작 전, 상단 메뉴 '파일' > '드라이브에 사본 저장' 을 선택하여 사본을 가지고 작업을 진행해주시기 바랍니다.*  

# 공지
- csv 파일 제출: dcc2021.clustering@gmail.com (발표 자료는 다른 곳으로 제출하셔야 합니다!!!)
- 리더 보드: https://github.com/High-East/DCC-12-Leader-Board
- 총 제출 횟수는 ***20번*** 입니다.
- 이메일을 보낸 시간을 기준으로, 대회 종료 시각인 ***PM 4:20*** 까지 제출하신 것까지만 인정합니다.
- 평가 지표는 ARI를 사용하고 있습니다. [-1, 1] 사이의 값을 가지며, 1에 가까울수록 높은 성능입니다.


# 기본 환경 세팅
1. **colab 파일**을 사본으로 저장해주세요.
2. [dataset](https://drive.google.com/file/d/1QH71L_uXYm0pRuMwubHIJgSix4y8Fi5I/view?usp=sharing)을 바로가기 추가해주세요.

# 문제
> Q. 2040년, 집에서도 인공지능이 도입된 로봇이 사용되기 시작했습니다. 집을 치워주는 로봇 "깔끔이"가 있습니다. 아쉽게도 "깔끔이"는 특정 물건이 어떤 것인지는 모릅니다. 하지만, A와 B가 같은 물건인지는 인식할 수 있습니다. "깔끔이"는 특히, 아이들이 사는 집에서 인기가 많습니다. 아이들이 어지럽힌 집을 "깔끔이"가 잘 치울 수 있도록 군집 알고리즘을 만들어보세요!


# 데이터 설명
훈련 데이터(Train Data): 637개의 레이블이 없는 사물 데이터  
평가 데이터(Test Data): 훈련 데이터와 동일  
피쳐 설명(Feature): (128, 128) 크기의 컬러 사진  
목표: 가장 적합한 군집의 개수를 찾고, 높은 군집 정확도 얻기  


# 구글 드라이브 마운트


```python
# 구글 드라이브를 마운트합니다.
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
```

    Mounted at /content/gdrive
    


```python
# 폴더 이동
import os
ROOT = '/content/gdrive/MyDrive'  # 변경하지 마세요. 단, MyDrive가 오류난다면, My\ Drive로 적으세요.
DIR = 'DCC2021'  # 데이터가 있는 위치를 적으세요.

PATH = os.path.join(ROOT, DIR)
os.chdir(PATH)
print(f"현재 경로: {PATH}")
```

    현재 경로: /content/gdrive/MyDrive/DCC2021
    


```python
# 할당 받은 GPU 확인
!nvidia-smi
```

    Tue Nov 16 12:46:13 2021       
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
    | N/A   69C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+
                                                                                   
    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |  No running processes found                                                 |
    +-----------------------------------------------------------------------------+
    

# 패키지 임포트


```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import models, layers, activations
from tensorflow.keras import optimizers
from sklearn.cluster import KMeans

import cv2
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist
import ipywidgets as widgets
from ipywidgets import interact

print(f"tensorflow version: {tf.__version__}")
print(f"keras version: {tf.keras.__version__}")
```

    tensorflow version: 2.7.0
    keras version: 2.7.0
    

# 커스텀 함수 정의


```python
# Visualization function
def show_img(dataset, predict, idx):
    fig = plt.figure()
    ax1 = fig.add_subplot(1, 2, 1)
    ax1.imshow(cv2.cvtColor(dataset[idx], cv2.COLOR_BGR2RGB))
    ax1.set_title("Raw image")
    ax2 = fig.add_subplot(1, 2, 2)
    ax2.imshow(cv2.cvtColor(predict[idx], cv2.COLOR_BGR2RGB))
    ax2.set_title("Reconstructed image")
    plt.show()

# Elbow method
def elbow_method(X, a, b):
    sse = []
    for i in range(a, b + 1):
        km = KMeans(n_clusters=i, init='k-means++', random_state=1207)
        km.fit(X)
        sse.append(km.inertia_)
    plt.figure(figsize=(8,6))
    plt.plot(range(a, b + 1), sse, marker='o', c='black')
    plt.xlabel("Number of clusters", fontsize=12)
    plt.ylabel("SSE", fontsize=12)
    plt.title("Elbow method", fontsize=15)
    plt.grid(linestyle='--')
    plt.show()
```

# 데이터 로드


```python
# 데이터 로드
path = "dataset.npy"
dataset = np.load(path)
dataset = np.float32(dataset / 255)  # 데이터 정규화

# 데이터 shape 확인
print(f"dataset shape: {dataset.shape}")
```

    dataset shape: (637, 128, 128, 3)
    


```python
# 이미지 확인
idx = 2
plt.imshow(cv2.cvtColor(dataset[idx], cv2.COLOR_BGR2RGB))
plt.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)
plt.show()
```


    
![png](output_15_0.png)
    


# 차원 축소
- PCA와 같은 다른 차원 축소 기법을 사용해도 괜찮습니다.

## 오토인코더


```python
# 인코더
encoder = models.Sequential([
          layers.InputLayer(input_shape=(128, 128, 3)),
          layers.Conv2D(filters=32,kernel_initializer='he_normal', kernel_size=3, strides=2, padding="same"),
          layers.BatchNormalization(),
          layers.ReLU(),
          layers.Conv2D(filters=32,kernel_initializer='he_normal', kernel_size=3, strides=2, padding="same"),
          layers.BatchNormalization(),
          layers.ReLU(),
          layers.Flatten(),
          layers.Dense(units=128, activation='relu')
          ],
          name='Encoder')

# 모델 구조 시각화
encoder.summary()
```

    Model: "Encoder"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d (Conv2D)             (None, 64, 64, 32)        896       
                                                                     
     batch_normalization (BatchN  (None, 64, 64, 32)       128       
     ormalization)                                                   
                                                                     
     re_lu (ReLU)                (None, 64, 64, 32)        0         
                                                                     
     conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      
                                                                     
     batch_normalization_1 (Batc  (None, 32, 32, 32)       128       
     hNormalization)                                                 
                                                                     
     re_lu_1 (ReLU)              (None, 32, 32, 32)        0         
                                                                     
     flatten (Flatten)           (None, 32768)             0         
                                                                     
     dense (Dense)               (None, 128)               4194432   
                                                                     
    =================================================================
    Total params: 4,204,832
    Trainable params: 4,204,704
    Non-trainable params: 128
    _________________________________________________________________
    


```python
# 디코더
decoder = models.Sequential([
          layers.InputLayer(input_shape=(128)),  # 인코더의 마지막 output shape을 입력하세요.
          layers.Dense(units=512, activation='relu'),
          layers.Reshape(target_shape=(8, 8, 8)),
          layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding="same"),
          layers.BatchNormalization(),
          layers.ReLU(),
          layers.Conv2DTranspose(filters=16, kernel_size=3, strides=2, padding="same"),
          layers.BatchNormalization(),
          layers.ReLU(),
          layers.UpSampling2D(),
          layers.UpSampling2D(),
          layers.Conv2D(filters=3, kernel_size=3, strides=1, activation='tanh', padding="same"),
         ],
          name='Decoder')

# 모델 구조 시각화
decoder.summary()
```

    Model: "Decoder"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     dense_1 (Dense)             (None, 512)               66048     
                                                                     
     reshape (Reshape)           (None, 8, 8, 8)           0         
                                                                     
     conv2d_transpose (Conv2DTra  (None, 16, 16, 32)       2336      
     nspose)                                                         
                                                                     
     batch_normalization_2 (Batc  (None, 16, 16, 32)       128       
     hNormalization)                                                 
                                                                     
     re_lu_2 (ReLU)              (None, 16, 16, 32)        0         
                                                                     
     conv2d_transpose_1 (Conv2DT  (None, 32, 32, 16)       4624      
     ranspose)                                                       
                                                                     
     batch_normalization_3 (Batc  (None, 32, 32, 16)       64        
     hNormalization)                                                 
                                                                     
     re_lu_3 (ReLU)              (None, 32, 32, 16)        0         
                                                                     
     up_sampling2d (UpSampling2D  (None, 64, 64, 16)       0         
     )                                                               
                                                                     
     up_sampling2d_1 (UpSampling  (None, 128, 128, 16)     0         
     2D)                                                             
                                                                     
     conv2d_2 (Conv2D)           (None, 128, 128, 3)       435       
                                                                     
    =================================================================
    Total params: 73,635
    Trainable params: 73,539
    Non-trainable params: 96
    _________________________________________________________________
    


```python
# 오토인코더: 인코더 + 디코더 구조

autoencoder = models.Sequential([
              layers.InputLayer(input_shape=(128, 128, 3), name='Input'),
              encoder,  # 인코더
              decoder])  # 디코더
```

## 학습
- Optimizer, loss 및 모든 하이퍼파라미터를 자유롭게 조정하며 실험해보세요.
- Optimizer 참고 자료 1 : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
- Optimizer 참고 자료 2: https://keras.io/ko/optimizers/
- Loss 참고자료 1: https://www.tensorflow.org/api_docs/python/tf/keras/losses
- Loss 참고자료 2: https://keras.io/ko/losses/


```python
# 하이퍼파라미터 설정
lr = 0.00001
optimizer = 'adam'
loss = 'msle'
epochs = 200
batch_size = 16

# 모델 컴파일(=optimizer, loss 세팅)
autoencoder.compile(optimizer=optimizer,
                    loss=loss)

# 학습
autoencoder.fit(x=dataset, y=dataset, epochs=epochs, batch_size=batch_size)
```

    Epoch 1/200
    40/40 [==============================] - 32s 29ms/step - loss: 0.0365
    Epoch 2/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0159
    Epoch 3/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0121
    Epoch 4/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0102
    Epoch 5/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0092
    Epoch 6/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0083
    Epoch 7/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0071
    Epoch 8/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0062
    Epoch 9/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0058
    Epoch 10/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0055
    Epoch 11/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0053
    Epoch 12/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0051
    Epoch 13/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0050
    Epoch 14/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0049
    Epoch 15/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0047
    Epoch 16/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0046
    Epoch 17/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0046
    Epoch 18/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0044
    Epoch 19/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0044
    Epoch 20/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0044
    Epoch 21/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0043
    Epoch 22/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0043
    Epoch 23/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0042
    Epoch 24/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0041
    Epoch 25/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0041
    Epoch 26/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0040
    Epoch 27/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0040
    Epoch 28/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0040
    Epoch 29/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0039
    Epoch 30/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0039
    Epoch 31/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0039
    Epoch 32/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0037
    Epoch 33/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0037
    Epoch 34/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0037
    Epoch 35/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0037
    Epoch 36/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0037
    Epoch 37/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0036
    Epoch 38/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0035
    Epoch 39/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0035
    Epoch 40/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0035
    Epoch 41/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0035
    Epoch 42/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0035
    Epoch 43/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0035
    Epoch 44/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0034
    Epoch 45/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0033
    Epoch 46/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0033
    Epoch 47/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0033
    Epoch 48/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0033
    Epoch 49/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0033
    Epoch 50/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0032
    Epoch 51/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0032
    Epoch 52/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0032
    Epoch 53/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0031
    Epoch 54/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0031
    Epoch 55/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0032
    Epoch 56/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0031
    Epoch 57/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0031
    Epoch 58/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0031
    Epoch 59/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0031
    Epoch 60/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0031
    Epoch 61/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0030
    Epoch 62/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0030
    Epoch 63/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0031
    Epoch 64/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0031
    Epoch 65/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0030
    Epoch 66/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 67/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 68/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0029
    Epoch 69/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0030
    Epoch 70/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0030
    Epoch 71/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 72/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0029
    Epoch 73/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 74/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0029
    Epoch 75/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 76/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 77/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0029
    Epoch 78/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 79/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0028
    Epoch 80/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0028
    Epoch 81/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0029
    Epoch 82/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0028
    Epoch 83/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0028
    Epoch 84/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0028
    Epoch 85/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0028
    Epoch 86/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 87/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 88/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 89/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0028
    Epoch 90/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 91/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 92/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 93/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 94/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 95/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 96/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 97/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 98/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 99/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 100/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0026
    Epoch 101/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 102/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 103/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 104/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0027
    Epoch 105/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 106/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 107/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 108/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 109/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 110/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0027
    Epoch 111/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 112/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0026
    Epoch 113/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 114/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 115/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 116/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 117/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 118/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 119/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 120/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 121/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 122/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 123/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 124/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 125/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 126/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 127/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 128/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 129/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 130/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 131/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 132/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 133/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 134/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 135/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 136/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 137/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 138/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 139/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 140/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 141/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 142/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 143/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0026
    Epoch 144/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0025
    Epoch 145/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 146/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0025
    Epoch 147/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 148/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 149/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 150/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 151/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 152/200
    40/40 [==============================] - 1s 25ms/step - loss: 0.0025
    Epoch 153/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 154/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 155/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 156/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 157/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 158/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 159/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 160/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 161/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 162/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 163/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 164/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 165/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 166/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 167/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 168/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 169/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 170/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 171/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 172/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0024
    Epoch 173/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 174/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 175/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 176/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 177/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 178/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0024
    Epoch 179/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 180/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 181/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 182/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 183/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 184/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 185/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 186/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 187/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 188/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 189/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 190/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 191/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 192/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 193/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 194/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 195/200
    40/40 [==============================] - 1s 25ms/step - loss: 0.0023
    Epoch 196/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0022
    Epoch 197/200
    40/40 [==============================] - 1s 23ms/step - loss: 0.0023
    Epoch 198/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    Epoch 199/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0022
    Epoch 200/200
    40/40 [==============================] - 1s 24ms/step - loss: 0.0023
    




    <keras.callbacks.History at 0x7f43f09269d0>




```python

```

## 복원된 이미지 확인


```python
# 오토인코더를 통해 얻은 이미지(=복원된 이미지)
max_len = 50
predict = autoencoder(dataset[:max_len])

# 시각화
slider = widgets.IntSlider(min=0, max=max_len, step=1, value=0)
interact(lambda idx: show_img(dataset[:max_len], predict.numpy(), idx), idx=slider)
```


    interactive(children=(IntSlider(value=0, description='idx', max=50), Output()), _dom_classes=('widget-interact…





    <function __main__.<lambda>>



# 클러스터링(K-means)
- 클러스터링 기법 변경하셔도 상관 없습니다.

## Latent vector 생성


```python
# Extract latent vector(mini-batch version, OOM 방지)
batch_size = 64
latent_vec = []
num_batch = len(dataset) // batch_size

for i in range(num_batch + 1):
    batch_set = dataset[i * batch_size:(i + 1) * batch_size]
    latent_vec.append(encoder(batch_set).numpy())

latent_vec = np.concatenate(latent_vec, axis=0)

if np.ndim(latent_vec) != 2:
    latent_vec = np.reshape(latent_vec, [len(dataset), -1])
```

## Cluster 개수 결정


```python
# Elbow method를 통해서 최적의 cluster 개수(K)를 찾아보세요.
# K는 15이하입니다.
# latent_vec의 크기가 클수록 런타임이 오래 걸리니 참고하세요.
start = 1
end = 15
elbow_method(latent_vec, start, end)
```


    
![png](output_30_0.png)
    



```python
# K-Means clustering
n_clusters = 10  # elbow method 등을 이용해서 K를 결정해보세요.
kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=1207).fit(latent_vec)
y_pred = kmeans.labels_
```

# 제출 파일(csv) 생성


```python
# 저장할 폴더 만들기
SAVE_PATH = os.path.join(PATH, "output")
if not os.path.exists(SAVE_PATH):
  os.mkdir(SAVE_PATH)
else:
  pass
```


```python
# 제출 파일 만들기
team = "데이터버스10"  # 반드시 자신이 속한 팀명을 적어주세요.
suffix = 1  # 자신이 제출한 파일을 알아볼 수 있는 접미사를 적어주세요. 이전에 제출한 파일과 동일하면 안됩니다.

# 이 밑은 수정하지 마세요.
# timestamp = datetime.today().strftime("%Y%m%d%H%M%S")
sub = pd.DataFrame(y_pred, columns=["y_pred"])
sub.to_csv(os.path.join(SAVE_PATH, f"./{team}_{suffix}.csv"), index=False)
```

# 제출
1. output 폴더 안에 있는 제출할 csv을 다운받아 주세요.
2. dcc2021.clustering@gmail.com로 csv파일을 첨부해서 보내주세요. (제목과 내용은 상관 없음)
3. 1-2분 후에 [리더보드](https://github.com/High-East/DCC-12-Leader-Board)에서 확인해주세요.


# 재공지
- 총 제출 횟수는 ***20번*** 입니다.
- 이메일을 보낸 시간을 기준으로, 대회 종료 시각인 ***PM 4:20*** 까지 제출하신 것까지만 인정합니다.
- 평가 지표는 ARI를 사용하고 있습니다. [-1, 1] 사이의 값을 가지며, 1에 가까울수록 높은 성능입니다.
- 발표 자료는 추후에 공지되는 이메일로 보내주세요.


```python

```
